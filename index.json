[{"content":"\nNVIDIA Isaac Sim with NVIDIA RTX PRO 4000 Blackwell: My New Robot Playground So, I finally did it. I caved in and bought the NVIDIA RTX PRO 4000 Blackwell GPU for my workstation. And yes, I’ll admit it: the main reason was that I wanted to generate some really big, high‑quality anime images (you know what I mean, lol).\nBut once I got it, I realized this thing is basically a tiny AI supercomputer, and it can run way more than just image generation, like NVIDIA Isaac Sim, which is my go‑to tool for robotics simulation and AI experiments.\nIn this post, I’ll walk you through my setup, the fun (and slightly painful) driver dance, and how I finally got Isaac Sim 5.1 running smoothly on this shiny new Blackwell beast.\nWhy I Bought the RTX PRO 4000 Blackwell I’ve been using Stable Diffusion and similar tools for a while, but with my old RTX 3060 Ti, generating a 4K anime image with a good model is impossible, even if 1K image is quite impossible and sometimes my GPU would scream in protest.\nWhen NVIDIA announced the RTX PRO 4000 Blackwell, I saw the specs: 24 GB of VRAM, Blackwell architecture, and insane tensor performance for generative AI. I thought: “This is the card that can finally let me generate those huge, detailed anime scenes in seconds, while still being powerful enough for serious robotics work”.\nSo, after a lot of internal debate (and checking my bank account), I pulled the trigger.\nMy Dual‑GPU Setup: Blackwell + 3060 Ti Instead of just replacing my old GPU, I decided to go for a dual‑GPU setup:\nNVIDIA RTX PRO 4000 Blackwell → Dedicated to AI, generative models, and Isaac Sim NVIDIA GeForce RTX 3060 Ti → Handles gaming, general desktop, and display output This way, I can:\nRun Isaac Sim and heavy AI workloads on the Blackwell without affecting my gaming FPS Keep my existing games and daily apps running smoothly on the 3060 Ti Use the Blackwell’s 24 GB VRAM for large models and high‑res simulations Physically, I just installed the RTX PRO 4000 Blackwell in a PCIe x16 slot, connected it to power, and left the 3060 Ti in its place. In BIOS, I set the 3060 Ti as the primary display GPU, so my monitors stay on it while the Blackwell crunches numbers in the background.\nThe Driver Dance: One Version to Rule Them Both Here’s where things got a little spicy. The RTX PRO 4000 Blackwell is a very new workstation GPU, and the recommended driver version for Isaac Sim 4.5 is 535.x, which doesn’t support Blackwell yet.\nBut there’s a catch: you can’t run different NVIDIA driver versions for two GPUs on the same system. You have to pick one driver version that works for both cards.\nSo I had to:\nUninstall the old 535 driver completely Install a newer driver that supports both the RTX PRO 4000 Blackwell and my RTX 3060 Ti Test stability with both gaming and Isaac Sim workloads After some trial and error (and a few crashes), I settled on NVIDIA driver version 580.105.08. This version:\nFully supports the RTX PRO 4000 Blackwell Works fine with the RTX 3060 Ti for gaming and display Is compatible with Isaac Sim 5.1, which is what I ended up using Pro tip: Always check the Isaac Sim Requirements page to see which driver version is recommended for your Isaac Sim version.\nPS. You can find full documentation abuot NVIDIA Driver Installation Guide from here.\nPSS: it seem like the first RTX PRO 4000 Blackwell supported version is 570.169.\nWhat Is NVIDIA Isaac Sim? (In a Nutshell) For those who aren’t deep into robotics yet, NVIDIA Isaac Sim is a high‑fidelity robotics simulator built on NVIDIA Omniverse. It lets you:\nCreate realistic 3D environments (factories, warehouses, labs, etc.) Simulate robots (UR5, Panda, custom robots) with physics and sensors Generate synthetic data (RGB, depth, LiDAR, IMU) for training AI models Test navigation, manipulation, and reinforcement learning policies before deploying to real robots In short: it’s like a “game engine for robots,” but with real physics, sensors, and AI integration.\nInstalling Isaac Sim on a Local PC (Step by Step) Here’s how I installed Isaac Sim on my Linux workstation (the steps are similar on Windows):\n1. Check System Requirements Before installing, I made sure my system met the requirements:\nOS: Ubuntu 22.04 / 24.04 (or Windows 10/11) CPU: Intel Core i7 / AMD Ryzen 7 or better RAM: 32 GB minimum, 64 GB recommended GPU: RTX 4080 / RTX PRO 4000 Blackwell or better, with 16+ GB VRAM Storage: 50+ GB SSD for the base install, more if you plan to run large scenes 2. Download Isaac Sim I went to the official Isaac Sim download page and grabbed the standalone workstation version for Linux. I chose the .zip package so I could install it in a custom location.\n3. Extract and Run the Installer In the terminal, I did something like this:\nmkdir ~/isaacsim cd ~/Downloads unzip \u0026#34;isaac-sim-standalone-5.1.0-linux-x86_64.zip\u0026#34; -d ~/isaacsim cd ~/isaacsim ./post_install.sh ./isaac-sim.selector.sh This creates the Isaac Sim App Selector, where I can choose which version and extensions to run.\n4. Launch Isaac Sim In the App Selector, I selected Isaac Sim Full and clicked START. The first launch takes a while because it’s warming up the shader cache, but after that, it starts much faster.\nThe Isaac Sim 4.5 vs. 5.1 Problem Here’s the big gotcha I ran into: Isaac Sim 4.5 officially recommends driver version 535, but that driver doesn’t support the RTX PRO 4000 Blackwell yet.\nSo, if you try to run Isaac Sim 4.5 on a Blackwell GPU with a newer driver, you’ll likely see:\nPoor performance Crashes or rendering issues Missing features or extensions The solution? Skip Isaac Sim 4.5 and go straight to Isaac Sim 5.0 or later. Isaac Sim 5.1 is designed to work with newer GPUs and drivers, including Blackwell.\nReinstalling from 4.5 to 5.1, and \u0026ldquo;voila!\u0026rdquo; Since I had been using Isaac Sim 4.5 before, I decided to do a clean reinstall:\nBacked up my important scenes and configs Removed the old Isaac Sim 4.5 installation folder Downloaded Isaac Sim 5.1 and installed it fresh in ~/isaacsim Ran ./post_install.sh and ./isaac-sim.selector.sh to set it up In case of 3D rendering is not working properly, set multi-GPU option to false with isaacsim setting or export CUDA_VISIBLE_DEVIES=\u0026lt;your gpu id\u0026gt; or run ./isaac-sim.sh --active_multi_gpu=false --/renderer/activeGpu=\u0026lt;your gpu id\u0026gt;. If it still not working, try to downgrade the driver version below 580. After that, I launched Isaac Sim 5.1, and… voila! Everything worked smoothly:\nThe Blackwell GPU was fully utilized Rendering was buttery smooth, even in complex scenes All the Isaac Sim extensions (ROS2 bridge, Isaac Lab, etc.) loaded without issues Tips for Running Isaac Sim 5.1 on RTX PRO 4000 Blackwell If you’re setting up a similar system, here are a few tips that helped me:\n1. Use the Compatibility Checker Isaac Sim includes a Compatibility Checker tool that verifies your GPU, driver, CPU, RAM, and storage. Run it before launching the full app to catch any issues early.\n2. Warm Up the Shader Cache The first time you run Isaac Sim, it can take 5–10 minutes to warm up the shader cache. After that, startup is much faster. If things feel slow, let it run for a few minutes and then restart.\n3. Use the Right Isaac Sim Version For RTX PRO 4000 Blackwell, stick with Isaac Sim 5.0 or later. Isaac Sim 4.5 is great for older GPUs, but it’s not the best choice for Blackwell.\n4. Offload Heavy Work to the Blackwell In your Isaac Sim projects, make sure to:\nUse the RTX PRO 4000 Blackwell for rendering and simulation Keep the RTX 3060 Ti for display and lighter tasks Monitor GPU usage with tools like nvidia-smi to balance the load Final Thoughts: Why This Setup Rocks Upgrading to the RTX PRO 4000 Blackwell has been a game‑changer for me. Now I can:\nGenerate huge anime images in seconds, not minutes (May be I should will make the blog about that in the next time?) Run Isaac Sim 5.1 with complex scenes and multiple robots at high frame rates Experiment with large models and reinforcement learning without constantly hitting VRAM limits If you’re planning a similar dual‑GPU setup (Blackwell for AI + an older GPU for gaming), my advice is:\nPick a driver version that supports both GPUs (I used 580.105.08) Use Isaac Sim 5.0+ for Blackwell GPUs And don’t forget to enjoy those big, beautiful anime images along the way, hehe Happy simulating (and generating)!\n","permalink":"https://bandhit.github.io/posts/isaac-sim-5.1-4000-blackwell/","summary":"\u003cp\u003e\u003cimg alt=\"4000-blackwell-001\" loading=\"lazy\" src=\"/posts/isaac-sim-5.1-4000-blackwell/4000-blackwell-001.png#center\"\u003e\u003c/p\u003e\n\u003ch2 id=\"nvidia-isaac-sim-with-nvidia-rtx-pro-4000-blackwell-my-new-robot-playground\"\u003eNVIDIA Isaac Sim with NVIDIA RTX PRO 4000 Blackwell: My New Robot Playground\u003c/h2\u003e\n\u003cp\u003eSo, I finally did it.\nI caved in and bought the \u003cstrong\u003eNVIDIA RTX PRO 4000 Blackwell\u003c/strong\u003e GPU for my workstation.\nAnd yes, I’ll admit it: the main reason was that I wanted to generate some \u003cem\u003ereally\u003c/em\u003e big, high‑quality anime images (you know what I mean, lol).\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"meme\" loading=\"lazy\" src=\"/posts/isaac-sim-5.1-4000-blackwell/meme.png#center\"\u003e\u003c/p\u003e\n\u003cp\u003eBut once I got it, I realized this thing is basically a tiny AI supercomputer, and it can run way more than just image generation, like \u003cstrong\u003eNVIDIA Isaac Sim\u003c/strong\u003e, which is my go‑to tool for robotics simulation and AI experiments.\u003c/p\u003e","title":"NVIDIA Isaac Sim with NVIDIA RTX PRO 4000 Blackwell"},{"content":"Motivation I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine. And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\nInstall Ollama on Your Machine Ollama works on Linux, macOS, and Windows. The easiest way to install on Linux or macOS is to open your terminal and run:\ncurl -fsSL https://ollama.com/install.sh | sh Windows users can download the installer directly from Ollama\u0026rsquo;s official website or run it inside WSL2 for more control.\nAfter installation, verify it’s all set:\nollama -v You should see the version printed out.\nOf course, you can install ollama on mamba as well, matter of preference. To install it using mamba CLI starting from creating a conda environment, you can follow these steps:\nCreate a new conda environment:\nmamba create -n ollama_env -y Activate the new environment:\nmamba activate ollama_env Install Hugo using mamba from the conda-forge channel:\nmamba install -c conda-forge ollama -y Pick and Pull Your Favorite AI Model Ollama has a built-in repository of open models like Llama 3, Mistral, and Phi-4.\nThis is some popular Ollama models with their size and CLI names, based on the most recent data available:\nModel Name Size CLI Name Gemma 3 (4B) 3.3GB gemma3 DeepSeek-R1 (7B) 4.7GB deepseek-r1 Llama 4 (109B) 67GB llama4:scout Llama 3.3 (70B) 43GB llama3.3 Llama 3.2 (3B) 2.0GB llama3.2 Llama 3.1 (8B) 4.7GB llama3.1 Phi 4 (14B) 9.1GB phi4 Mistral (7B) 4.1GB mistral Llama 2 Uncensored (7B) 3.8GB llama2-uncensored Granite-3.3 (8B) 4.9GB granite3.3 Want to try Llama 3.2, for example?\nFirstly, run:\nollama serve Next, open an another terminal, run:\nollama run llama3.2 This will download and launch the model on your machine. Be patient; these files can be a couple of gigabytes!\nAt this point, you can start chat to your Llama 3.2 via the current terminal here:\nPro. tips, if you are using NVIDIA GPU, you nvidia-smi to check resource usage on your GPU.\nFrom above, memory usage is 3637MiB of 8192MiB, since Llama 3.2 model size is just 2.0GB so it be fine for now.\nSet Up the Open WebUI Front-End While Ollama runs models via command line, Open WebUI adds a user-friendly browser interface.\nHere’s how:\nMake sure you have Docker installed. Run the Open WebUI Docker container with: docker run --network=host -p 3000:8080 -e WEBUI_AUTH=False -e OLLAMA_BASE_URL=http://127.0.0.1:11434 -v my-open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main Now, open your browser and go to http://localhost:8080/. You can now chat with your Ollama-powered LLM through the web!\nChat, Experiment, and Have Fun Use the web UI to try prompts, experiment with different models, or even upload documents for processing. Running AI locally means your data stays private, and you get full control over customization.\nRunning Ollama locally is like having a mini AI research lab right on your desk. Whether you\u0026rsquo;re a developer, hobbyist, or just curious about AI, it’s a rewarding experience to see LLMs work in real-time on your own machine.\nHappy AI-ing!\n","permalink":"https://bandhit.github.io/posts/ollama-local-1/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI saw this recently: \u003ca href=\"https://www.youtube.com/watch?v=qw4fDU18RcU\"\u003ePewDiePie built his own chat app to run LLMs locally on his machine.\u003c/a\u003e\nAnd also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\u003c/p\u003e","title":"Run Ollama Locally with a Web UI"},{"content":"Motivation Someone came up to me and said, \u0026ldquo;Let\u0026rsquo;s make some blogs!\u0026rdquo; At first, my reaction was \u0026hellip;\nOk, then, to do that, my instinct was to run the other way. Anything to avoid being trapped in the world of WordPress themes or WIX wizardry. Instead, I dusted off my terminal, grabbed a cup of cocoa, and dove into Hugo-a lightning-fast static site generator that lets me control every pixel. For hosting, no pricey managed platforms, just a trusty Microsoft Azure Blob Storage account, letting me deploy and update with just a push. Now, my blog is blazing fast, totally customizable, and blissfully free of cookie-cutter templates. Who knew building websites could actually be empowering (and kind of fun)?\nThis tutorial guides you through creating a blog using Hugo, a fast and flexible static site generator, and deploying it with Microsoft Azure Blob Storage.\nPrerequisites Install Hugo Install Git Have an active Microsoft Azure account with Blob Storage setup Basic command-line and text editor familiarity Install Hugo Tips With snap (for Ubuntu) To install Hugo with snap, run:\nsudo snap install hugo With mamba (miniforge system) Personally, I prefer using mamba for package management across multiple-projects. Because it\u0026rsquo;s much faster and less prone to dependency resolution issues. Also, if you’re working in a commercial or government environment, don’t install Anaconda unless your organization has actually purchased a license—otherwise, you could be violating terms of use. Community packages like mamba are free for anyone to use without commercial restrictions.\nTo install Hugo using mamba CLI starting from creating a conda environment, you can follow these steps:\nCreate a new conda environment:\nmamba create -n hugo_env -y Activate the new environment:\nmamba activate hugo_env Install Hugo using mamba from the conda-forge channel:\nmamba install -c conda-forge hugo -y This sequence sets up a mamba environment and installs Hugo efficiently.\nCreate a New Hugo Site Open your terminal and run:\nhugo new site mlablogs cd mlablogs git init This sets up a fresh Hugo project.\nAdd a Theme Choose a theme from the Hugo themes repository. For example, add the Ananke theme:\ngit submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod echo \u0026#39;theme = \u0026#34;PaperMod\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml You can find more themes on . You can also change the theme later by editing config.toml, and don\u0026rsquo;t forget to verify your submodules in .gitmodules if you need to remove or add more themes.\nAdd Content Create your first blog post:\nmkdir -p content/posts/hello-hugo nano content/posts/hello-hugo/index.md Edit this Markdown file and add your text.\n+++ date = \u0026#39;2025-11-03T12:00:00+09:00\u0026#39; draft = false title = \u0026#39;Hello Hugo\u0026#39; +++ ## Hello Hugo! Hello Hugo! Preview Locally Run:\nhugo server Visit http://localhost:1313 to see your site live locally.\nVery simple, isn’t it?\nBuild the Site Generate static files:\nhugo Static files output to the public directory.\nDeploy to Azure Blob Storage Create a Blob Storage container in Azure Portal and set it for static website hosting. Use Azure CLI or Azure Storage Explorer to upload the contents of the public folder to the $web container. Configure your storage account\u0026rsquo;s static website endpoint as your site URL. There you have it There you have it. Now you know it\u0026hellip; no refunds if it stops working!\n","permalink":"https://bandhit.github.io/posts/hello-hugo/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eSomeone came up to me and said, \u0026ldquo;Let\u0026rsquo;s make some blogs!\u0026rdquo;\nAt first, my reaction was \u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"aufkm\" loading=\"lazy\" src=\"/posts/hello-hugo/aufkm.png#center\"\u003e\u003c/p\u003e\n\u003cp\u003eOk, then, to do that, my instinct was to run the other way. Anything to avoid being trapped in the world of WordPress themes or WIX wizardry.\nInstead, I dusted off my terminal, grabbed a cup of cocoa, and dove into Hugo-a lightning-fast static site generator that lets me control every pixel.\nFor hosting, no pricey managed platforms, just a trusty Microsoft Azure Blob Storage account, letting me deploy and update with just a push.\nNow, my blog is blazing fast, totally customizable, and blissfully free of cookie-cutter templates.\nWho knew building websites could actually be empowering (and kind of fun)?\u003c/p\u003e","title":"Quick Tutorial: Creating Blogs and Websites with Hugo + Microsoft Azure Blob Storage Account"},{"content":"\nWelcome aboard, monke.\nReference: Return to Monke\nThis is MLablogs, my little corner of the internet where experiments and thoughts spill out like half-written code. You’ll find random lab notes, half-finished tech ideas, and the occasional story that slipped past the debug console. Have fun.\n","permalink":"https://bandhit.github.io/posts/welcome-monke/","summary":"\u003cp\u003e\u003cimg alt=\"monke\" loading=\"lazy\" src=\"/posts/welcome-monke/monke.jpg\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWelcome aboard, monke.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://knowyourmeme.com/memes/return-to-monke\"\u003e\u003cem\u003eReference: Return to Monke\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is MLablogs, my little corner of the internet where experiments and thoughts spill out like half-written code.\nYou’ll find random lab notes, half-finished tech ideas, and the occasional story that slipped past the debug console.\nHave fun.\u003c/p\u003e","title":"Welcome Aboard"}]