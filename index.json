[{"content":"Motivation I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine. And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\nInstall Ollama on Your Machine Ollama works on Linux, macOS, and Windows. The easiest way to install on Linux or macOS is to open your terminal and run:\ncurl -fsSL https://ollama.com/install.sh | sh Windows users can download the installer directly from Ollama\u0026rsquo;s official website or run it inside WSL2 for more control.\nAfter installation, verify it’s all set:\nollama -v You should see the version printed out.\nOf course, you can install ollama on mamba as well, matter of preference. To install it using mamba CLI starting from creating a conda environment, you can follow these steps:\nCreate a new conda environment:\nmamba create -n ollama_env -y Activate the new environment:\nmamba activate ollama_env Install Hugo using mamba from the conda-forge channel:\nmamba install -c conda-forge ollama -y Pick and Pull Your Favorite AI Model Ollama has a built-in repository of open models like Llama 3, Mistral, and Phi-4.\nThis is some popular Ollama models with their size and CLI names, based on the most recent data available:\nModel Name Size CLI Name Gemma 3 (4B) 3.3GB gemma3 DeepSeek-R1 (7B) 4.7GB deepseek-r1 Llama 4 (109B) 67GB llama4:scout Llama 3.3 (70B) 43GB llama3.3 Llama 3.2 (3B) 2.0GB llama3.2 Llama 3.1 (8B) 4.7GB llama3.1 Phi 4 (14B) 9.1GB phi4 Mistral (7B) 4.1GB mistral Llama 2 Uncensored (7B) 3.8GB llama2-uncensored Granite-3.3 (8B) 4.9GB granite3.3 Want to try Llama 3.2, for example?\nFirstly, run:\nollama serve Next, open an another terminal, run:\nollama run llama3.2 This will download and launch the model on your machine. Be patient; these files can be a couple of gigabytes!\nAt this point, you can start chat to your Llama 3.2 via the current terminal here:\nPro. tips, if you are using NVIDIA GPU, you nvidia-smi to check resource usage on your GPU.\nFrom above, memory usage is 3637MiB of 8192MiB, since Llama 3.2 model size is just 2.0GB so it be fine for now.\nSet Up the Open WebUI Front-End While Ollama runs models via command line, Open WebUI adds a user-friendly browser interface.\nHere’s how:\nMake sure you have Docker installed. Run the Open WebUI Docker container with: docker run --network=host -p 3000:8080 -e WEBUI_AUTH=False -e OLLAMA_BASE_URL=http://127.0.0.1:11434 -v my-open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main Now, open your browser and go to http://localhost:8080/. You can now chat with your Ollama-powered LLM through the web!\nChat, Experiment, and Have Fun Use the web UI to try prompts, experiment with different models, or even upload documents for processing. Running AI locally means your data stays private, and you get full control over customization.\nRunning Ollama locally is like having a mini AI research lab right on your desk. Whether you\u0026rsquo;re a developer, hobbyist, or just curious about AI, it’s a rewarding experience to see LLMs work in real-time on your own machine.\nHappy AI-ing!\n","permalink":"https://bandhit.github.io/posts/ollama-local-1/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eI saw this recently: \u003ca href=\"https://www.youtube.com/watch?v=qw4fDU18RcU\"\u003ePewDiePie built his own chat app to run LLMs locally on his machine.\u003c/a\u003e\nAnd also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\u003c/p\u003e","title":"Run Ollama Locally with a Web UI"},{"content":"Motivation Someone came up to me and said, \u0026ldquo;Let\u0026rsquo;s make some blogs!\u0026rdquo; At first, my reaction was \u0026hellip;\nOk, then, to do that, my instinct was to run the other way. Anything to avoid being trapped in the world of WordPress themes or WIX wizardry. Instead, I dusted off my terminal, grabbed a cup of cocoa, and dove into Hugo-a lightning-fast static site generator that lets me control every pixel. For hosting, no pricey managed platforms, just a trusty Microsoft Azure Blob Storage account, letting me deploy and update with just a push. Now, my blog is blazing fast, totally customizable, and blissfully free of cookie-cutter templates. Who knew building websites could actually be empowering (and kind of fun)?\nThis tutorial guides you through creating a blog using Hugo, a fast and flexible static site generator, and deploying it with Microsoft Azure Blob Storage.\nPrerequisites Install Hugo Install Git Have an active Microsoft Azure account with Blob Storage setup Basic command-line and text editor familiarity Install Hugo Tips With snap (for Ubuntu) To install Hugo with snap, run:\nsudo snap install hugo With mamba (miniforge system) Personally, I prefer using mamba for package management across multiple-projects. Because it\u0026rsquo;s much faster and less prone to dependency resolution issues. Also, if you’re working in a commercial or government environment, don’t install Anaconda unless your organization has actually purchased a license—otherwise, you could be violating terms of use. Community packages like mamba are free for anyone to use without commercial restrictions.\nTo install Hugo using mamba CLI starting from creating a conda environment, you can follow these steps:\nCreate a new conda environment:\nmamba create -n hugo_env -y Activate the new environment:\nmamba activate hugo_env Install Hugo using mamba from the conda-forge channel:\nmamba install -c conda-forge hugo -y This sequence sets up a mamba environment and installs Hugo efficiently.\nCreate a New Hugo Site Open your terminal and run:\nhugo new site mlablogs cd mlablogs git init This sets up a fresh Hugo project.\nAdd a Theme Choose a theme from the Hugo themes repository. For example, add the Ananke theme:\ngit submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod echo \u0026#39;theme = \u0026#34;PaperMod\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml You can find more themes on . You can also change the theme later by editing config.toml, and don\u0026rsquo;t forget to verify your submodules in .gitmodules if you need to remove or add more themes.\nAdd Content Create your first blog post:\nmkdir -p content/posts/hello-hugo nano content/posts/hello-hugo/index.md Edit this Markdown file and add your text.\n+++ date = \u0026#39;2025-11-03T12:00:00+09:00\u0026#39; draft = false title = \u0026#39;Hello Hugo\u0026#39; +++ ## Hello Hugo! Hello Hugo! Preview Locally Run:\nhugo server Visit http://localhost:1313 to see your site live locally.\nVery simple, isn’t it?\nBuild the Site Generate static files:\nhugo Static files output to the public directory.\nDeploy to Azure Blob Storage Create a Blob Storage container in Azure Portal and set it for static website hosting. Use Azure CLI or Azure Storage Explorer to upload the contents of the public folder to the $web container. Configure your storage account\u0026rsquo;s static website endpoint as your site URL. There you have it There you have it. Now you know it\u0026hellip; no refunds if it stops working!\n","permalink":"https://bandhit.github.io/posts/hello-hugo/","summary":"\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eSomeone came up to me and said, \u0026ldquo;Let\u0026rsquo;s make some blogs!\u0026rdquo;\nAt first, my reaction was \u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"aufkm\" loading=\"lazy\" src=\"/posts/hello-hugo/aufkm.png#center\"\u003e\u003c/p\u003e\n\u003cp\u003eOk, then, to do that, my instinct was to run the other way. Anything to avoid being trapped in the world of WordPress themes or WIX wizardry.\nInstead, I dusted off my terminal, grabbed a cup of cocoa, and dove into Hugo-a lightning-fast static site generator that lets me control every pixel.\nFor hosting, no pricey managed platforms, just a trusty Microsoft Azure Blob Storage account, letting me deploy and update with just a push.\nNow, my blog is blazing fast, totally customizable, and blissfully free of cookie-cutter templates.\nWho knew building websites could actually be empowering (and kind of fun)?\u003c/p\u003e","title":"Quick Tutorial: Creating Blogs and Websites with Hugo + Microsoft Azure Blob Storage Account"},{"content":"\nWelcome aboard, monke.\nReference: Return to Monke\nThis is MLablogs, my little corner of the internet where experiments and thoughts spill out like half-written code. You’ll find random lab notes, half-finished tech ideas, and the occasional story that slipped past the debug console. Have fun.\n","permalink":"https://bandhit.github.io/posts/welcome-monke/","summary":"\u003cp\u003e\u003cimg alt=\"monke\" loading=\"lazy\" src=\"/posts/welcome-monke/monke.jpg\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eWelcome aboard, monke.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ca href=\"https://knowyourmeme.com/memes/return-to-monke\"\u003e\u003cem\u003eReference: Return to Monke\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is MLablogs, my little corner of the internet where experiments and thoughts spill out like half-written code.\nYou’ll find random lab notes, half-finished tech ideas, and the occasional story that slipped past the debug console.\nHave fun.\u003c/p\u003e","title":"Welcome Aboard"}]