<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Run Ollama Locally with a Web UI | MLablogs</title>
<meta name="keywords" content="">
<meta name="description" content="Motivation
I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine.
And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!">
<meta name="author" content="Bandhit Suksiri">
<link rel="canonical" href="https://bandhit.github.io/posts/ollama-local-1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bandhit.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://bandhit.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://bandhit.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://bandhit.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://bandhit.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bandhit.github.io/posts/ollama-local-1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://bandhit.github.io/posts/ollama-local-1/">
  <meta property="og:site_name" content="MLablogs">
  <meta property="og:title" content="Run Ollama Locally with a Web UI">
  <meta property="og:description" content="Motivation I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine. And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!">
  <meta property="og:locale" content="en-US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-03T20:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-03T20:00:00+09:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Run Ollama Locally with a Web UI">
<meta name="twitter:description" content="Motivation
I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine.
And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bandhit.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Run Ollama Locally with a Web UI",
      "item": "https://bandhit.github.io/posts/ollama-local-1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Run Ollama Locally with a Web UI",
  "name": "Run Ollama Locally with a Web UI",
  "description": "Motivation I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine. And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\n",
  "keywords": [
    
  ],
  "articleBody": "Motivation I saw this recently: PewDiePie built his own chat app to run LLMs locally on his machine. And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!\nInstall Ollama on Your Machine Ollama works on Linux, macOS, and Windows. The easiest way to install on Linux or macOS is to open your terminal and run:\ncurl -fsSL https://ollama.com/install.sh | sh Windows users can download the installer directly from Ollama’s official website or run it inside WSL2 for more control.\nAfter installation, verify it’s all set:\nollama -v You should see the version printed out.\nOf course, you can install ollama on mamba as well, matter of preference. To install it using mamba CLI starting from creating a conda environment, you can follow these steps:\nCreate a new conda environment:\nmamba create -n ollama_env -y Activate the new environment:\nmamba activate ollama_env Install Hugo using mamba from the conda-forge channel:\nmamba install -c conda-forge ollama -y Pick and Pull Your Favorite AI Model Ollama has a built-in repository of open models like Llama 3, Mistral, and Phi-4.\nThis is some popular Ollama models with their size and CLI names, based on the most recent data available:\nModel Name Size CLI Name Gemma 3 (4B) 3.3GB gemma3 DeepSeek-R1 (7B) 4.7GB deepseek-r1 Llama 4 (109B) 67GB llama4:scout Llama 3.3 (70B) 43GB llama3.3 Llama 3.2 (3B) 2.0GB llama3.2 Llama 3.1 (8B) 4.7GB llama3.1 Phi 4 (14B) 9.1GB phi4 Mistral (7B) 4.1GB mistral Llama 2 Uncensored (7B) 3.8GB llama2-uncensored Granite-3.3 (8B) 4.9GB granite3.3 Want to try Llama 3.2, for example?\nFirstly, run:\nollama serve Next, open an another terminal, run:\nollama run llama3.2 This will download and launch the model on your machine. Be patient; these files can be a couple of gigabytes!\nAt this point, you can start chat to your Llama 3.2 via the current terminal here:\nPro. tips, if you are using NVIDIA GPU, you nvidia-smi to check resource usage on your GPU.\nFrom above, memory usage is 3637MiB of 8192MiB, since Llama 3.2 model size is just 2.0GB so it be fine for now.\nSet Up the Open WebUI Front-End While Ollama runs models via command line, Open WebUI adds a user-friendly browser interface.\nHere’s how:\nMake sure you have Docker installed. Run the Open WebUI Docker container with: docker run --network=host -p 3000:8080 -e WEBUI_AUTH=False -e OLLAMA_BASE_URL=http://127.0.0.1:11434 -v my-open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main Now, open your browser and go to http://localhost:8080/. You can now chat with your Ollama-powered LLM through the web!\nChat, Experiment, and Have Fun Use the web UI to try prompts, experiment with different models, or even upload documents for processing. Running AI locally means your data stays private, and you get full control over customization.\nRunning Ollama locally is like having a mini AI research lab right on your desk. Whether you’re a developer, hobbyist, or just curious about AI, it’s a rewarding experience to see LLMs work in real-time on your own machine.\nHappy AI-ing!\n",
  "wordCount" : "537",
  "inLanguage": "en",
  "datePublished": "2025-11-03T20:00:00+09:00",
  "dateModified": "2025-11-03T20:00:00+09:00",
  "author":{
    "@type": "Person",
    "name": "Bandhit Suksiri"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bandhit.github.io/posts/ollama-local-1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MLablogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bandhit.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bandhit.github.io/" accesskey="h" title="MLablogs (Alt + H)">MLablogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bandhit.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://bandhit.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Run Ollama Locally with a Web UI
    </h1>
    <div class="post-meta"><span title='2025-11-03 20:00:00 +0900 JST'>November 3, 2025</span>&nbsp;·&nbsp;<span>3 min</span>&nbsp;·&nbsp;<span>Bandhit Suksiri</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#motivation" aria-label="Motivation">Motivation</a><ul>
                        
                <li>
                    <a href="#install-ollama-on-your-machine" aria-label="Install Ollama on Your Machine">Install Ollama on Your Machine</a></li>
                <li>
                    <a href="#pick-and-pull-your-favorite-ai-model" aria-label="Pick and Pull Your Favorite AI Model">Pick and Pull Your Favorite AI Model</a></li>
                <li>
                    <a href="#set-up-the-open-webui-front-end" aria-label="Set Up the Open WebUI Front-End">Set Up the Open WebUI Front-End</a></li>
                <li>
                    <a href="#chat-experiment-and-have-fun" aria-label="Chat, Experiment, and Have Fun">Chat, Experiment, and Have Fun</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>I saw this recently: <a href="https://www.youtube.com/watch?v=qw4fDU18RcU">PewDiePie built his own chat app to run LLMs locally on his machine.</a>
And also, I’ve spent countless hours roasting my GPU with AAA games-might as well make it do something useful for a change! Have you ever wanted to play with large language models (LLMs) right on your own computer, without relying on the cloud? Enter Ollama, a nifty open-source tool that lets you run powerful AI models locally!</p>
<p><img alt="meme" loading="lazy" src="/posts/ollama-local-1/meme.png#center"></p>
<h3 id="install-ollama-on-your-machine">Install Ollama on Your Machine<a hidden class="anchor" aria-hidden="true" href="#install-ollama-on-your-machine">#</a></h3>
<p>Ollama works on Linux, macOS, and Windows. The easiest way to install on Linux or macOS is to open your terminal and run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl -fsSL https://ollama.com/install.sh <span class="p">|</span> sh
</span></span></code></pre></div><p>Windows users can download the installer directly from Ollama&rsquo;s official website or run it inside WSL2 for more control.</p>
<p>After installation, verify it’s all set:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ollama -v
</span></span></code></pre></div><p>You should see the version printed out.</p>
<p>Of course, you can install <code>ollama</code> on <code>mamba</code> as well, matter of preference.
To install it using <code>mamba</code> CLI starting from creating a conda environment, you can follow these steps:</p>
<p>Create a new conda environment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mamba create -n ollama_env -y
</span></span></code></pre></div><p>Activate the new environment:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mamba activate ollama_env
</span></span></code></pre></div><p>Install Hugo using mamba from the conda-forge channel:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mamba install -c conda-forge ollama -y
</span></span></code></pre></div><h3 id="pick-and-pull-your-favorite-ai-model">Pick and Pull Your Favorite AI Model<a hidden class="anchor" aria-hidden="true" href="#pick-and-pull-your-favorite-ai-model">#</a></h3>
<p>Ollama has a built-in repository of open models like Llama 3, Mistral, and Phi-4.</p>
<p>This is some popular Ollama models with their size and CLI names, based on the most recent data available:</p>
<table>
  <thead>
      <tr>
          <th>Model Name</th>
          <th>Size</th>
          <th>CLI Name</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Gemma 3 (4B)</td>
          <td>3.3GB</td>
          <td><code>gemma3</code></td>
      </tr>
      <tr>
          <td>DeepSeek-R1 (7B)</td>
          <td>4.7GB</td>
          <td><code>deepseek-r1</code></td>
      </tr>
      <tr>
          <td>Llama 4 (109B)</td>
          <td>67GB</td>
          <td><code>llama4:scout</code></td>
      </tr>
      <tr>
          <td>Llama 3.3 (70B)</td>
          <td>43GB</td>
          <td><code>llama3.3</code></td>
      </tr>
      <tr>
          <td>Llama 3.2 (3B)</td>
          <td>2.0GB</td>
          <td><code>llama3.2</code></td>
      </tr>
      <tr>
          <td>Llama 3.1 (8B)</td>
          <td>4.7GB</td>
          <td><code>llama3.1</code></td>
      </tr>
      <tr>
          <td>Phi 4 (14B)</td>
          <td>9.1GB</td>
          <td><code>phi4</code></td>
      </tr>
      <tr>
          <td>Mistral (7B)</td>
          <td>4.1GB</td>
          <td><code>mistral</code></td>
      </tr>
      <tr>
          <td>Llama 2 Uncensored (7B)</td>
          <td>3.8GB</td>
          <td><code>llama2-uncensored</code></td>
      </tr>
      <tr>
          <td>Granite-3.3 (8B)</td>
          <td>4.9GB</td>
          <td><code>granite3.3</code></td>
      </tr>
  </tbody>
</table>
<p>Want to try Llama 3.2, for example?</p>
<p>Firstly, run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ollama serve
</span></span></code></pre></div><p>Next, open an another terminal, run:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ollama run llama3.2
</span></span></code></pre></div><p>This will download and launch the model on your machine.
Be patient; these files can be a couple of gigabytes!</p>
<p><img alt="log1" loading="lazy" src="/posts/ollama-local-1/log1.png"></p>
<p>At this point, you can start chat to your Llama 3.2 via the current terminal here:</p>
<p><img alt="log2" loading="lazy" src="/posts/ollama-local-1/log2.png"></p>
<p>Pro. tips, if you are using NVIDIA GPU, you <code>nvidia-smi</code> to check resource usage on your GPU.</p>
<p><img alt="log3" loading="lazy" src="/posts/ollama-local-1/log3.png"></p>
<p>From above, memory usage is 3637MiB of 8192MiB, since Llama 3.2 model size is just 2.0GB so it be fine for now.</p>
<h3 id="set-up-the-open-webui-front-end">Set Up the Open WebUI Front-End<a hidden class="anchor" aria-hidden="true" href="#set-up-the-open-webui-front-end">#</a></h3>
<p>While Ollama runs models via command line, Open WebUI adds a user-friendly browser interface.</p>
<p>Here’s how:</p>
<ul>
<li>Make sure you have Docker installed.</li>
<li>Run the Open WebUI Docker container with:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker run --network<span class="o">=</span>host -p 3000:8080 -e <span class="nv">WEBUI_AUTH</span><span class="o">=</span>False -e <span class="nv">OLLAMA_BASE_URL</span><span class="o">=</span>http://127.0.0.1:11434 -v my-open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
</span></span></code></pre></div><p>Now, open your browser and go to <code>http://localhost:8080/</code>.
You can now chat with your Ollama-powered LLM through the web!</p>
<p><img alt="log4" loading="lazy" src="/posts/ollama-local-1/log4.png"></p>
<p><img alt="log5" loading="lazy" src="/posts/ollama-local-1/log5.png"></p>
<h3 id="chat-experiment-and-have-fun">Chat, Experiment, and Have Fun<a hidden class="anchor" aria-hidden="true" href="#chat-experiment-and-have-fun">#</a></h3>
<p>Use the web UI to try prompts, experiment with different models, or even upload documents for processing.
Running AI locally means your data stays private, and you get full control over customization.</p>
<p>Running Ollama locally is like having a mini AI research lab right on your desk.
Whether you&rsquo;re a developer, hobbyist, or just curious about AI, it’s a rewarding experience to see LLMs work in real-time on your own machine.</p>
<p>Happy AI-ing!</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on x"
            href="https://x.com/intent/tweet/?text=Run%20Ollama%20Locally%20with%20a%20Web%20UI&amp;url=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f&amp;title=Run%20Ollama%20Locally%20with%20a%20Web%20UI&amp;summary=Run%20Ollama%20Locally%20with%20a%20Web%20UI&amp;source=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f&title=Run%20Ollama%20Locally%20with%20a%20Web%20UI">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on whatsapp"
            href="https://api.whatsapp.com/send?text=Run%20Ollama%20Locally%20with%20a%20Web%20UI%20-%20https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on telegram"
            href="https://telegram.me/share/url?text=Run%20Ollama%20Locally%20with%20a%20Web%20UI&amp;url=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Run Ollama Locally with a Web UI on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Run%20Ollama%20Locally%20with%20a%20Web%20UI&u=https%3a%2f%2fbandhit.github.io%2fposts%2follama-local-1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© Bandhit Suksiri, D.Eng. in Inf. Syst.</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
